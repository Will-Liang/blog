(window.webpackJsonp=window.webpackJsonp||[]).push([[64],{464:function(s,t,a){"use strict";a.r(t);var e=a(2),r=Object(e.a)({},(function(){var s=this,t=s._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[t("h2",{attrs:{id:"_1-介绍scrapy-redis框架"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-介绍scrapy-redis框架"}},[s._v("#")]),s._v(" 1. 介绍scrapy-redis框架")]),s._v(" "),t("p",[s._v("scrapy-redis")]),s._v(" "),t("blockquote",[t("p",[s._v("一个三方的基于redis的分布式爬虫框架，配合scrapy使用，让爬虫具有了分布式爬取的功能。")])]),s._v(" "),t("p",[s._v("github地址：\nhttps://github.com/darkrho/scrapy-redis")]),s._v(" "),t("h2",{attrs:{id:"_2-分布式原理"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-分布式原理"}},[s._v("#")]),s._v(" 2. 分布式原理")]),s._v(" "),t("p",[s._v("scrapy-redis实现分布式，其实从原理上来说很简单，这里为描述方便，我们把自己的"),t("strong",[s._v("核心服务器")]),s._v("称为"),t("strong",[s._v("master")]),s._v("，而把用于"),t("strong",[s._v("跑爬虫程序")]),s._v("的机器称为"),t("strong",[s._v("slave")])]),s._v(" "),t("p",[s._v("我们知道，采用scrapy框架抓取网页，我们需要首先给定它一些start_urls，爬虫首先访问start_urls里面的url，再根据我们的具体逻辑，对里面的元素、或者是其他的二级、三级页面进行抓取。而要实现分布式，我们只需要在这个starts_urls里面做文章就行了")]),s._v(" "),t("p",[s._v("我们在"),t("strong",[s._v("master")]),s._v("上搭建一个"),t("strong",[s._v("redis数据库")]),s._v("（注意这个数据库只用作url的存储)，并对每一个需要爬取的网站类型，都开辟一个单独的列表字段。通过设置slave上scrapy-redis获取url的地址为master地址。这样的结果就是，"),t("strong",[s._v("尽管有多个slave，然而大家获取url的地方只有一个，那就是服务器master上的redis数据库")])]),s._v(" "),t("p",[s._v("并且，由于scrapy-redis"),t("strong",[s._v("自身的队列机制")]),s._v("，slave获取的链接不会相互冲突。这样各个slave在完成抓取任务之后，再把获取的结果汇总到服务器上")]),s._v(" "),t("p",[t("strong",[s._v("好处")])]),s._v(" "),t("p",[s._v("程序移植性强，只要处理好路径问题，把slave上的程序移植到另一台机器上运行，基本上就是复制粘贴的事情")]),s._v(" "),t("h2",{attrs:{id:"_3-分布式爬虫的实现"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-分布式爬虫的实现"}},[s._v("#")]),s._v(" 3.分布式爬虫的实现")]),s._v(" "),t("ol",[t("li",[t("p",[s._v("使用三台机器，一台是win10，两台是centos6，分别在两台机器上部署scrapy来进行分布式抓取一个网站")])]),s._v(" "),t("li",[t("p",[s._v("win10的ip地址为192.168.31.245，用来作为redis的master端，centos的机器作为slave")])]),s._v(" "),t("li",[t("p",[s._v("master的爬虫运行时会把提取到的url封装成request放到redis中的数据库：“dmoz(项目名):requests”，并且从该数据库中提取request后下载网页，再把网页的内容存放到redis的另一个数据库中“dmoz(项目名):items”")])]),s._v(" "),t("li",[t("p",[s._v("slave从master的redis中取出待抓取的request，下载完网页之后就把网页的内容发送回master的redis")])]),s._v(" "),t("li",[t("p",[s._v("重复上面的3和4，直到master的redis中的“dmoz:requests”数据库为空，再把master的redis中的“dmoz:items”数据库写入到mongodb中")])]),s._v(" "),t("li",[t("p",[s._v("master里的reids还有一个数据“dmoz:dupefilter”是用来存储抓取过的url的指纹（使用哈希函数将url运算后的结果），是防止重复抓取的")])])]),s._v(" "),t("h2",{attrs:{id:"_4-scrapy-redis框架的安装"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-scrapy-redis框架的安装"}},[s._v("#")]),s._v(" 4. scrapy-redis框架的安装")]),s._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("pip install scrapy-redis\n")])])]),t("h2",{attrs:{id:"_5-部署scrapy-redis"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-部署scrapy-redis"}},[s._v("#")]),s._v(" 5. 部署scrapy-redis")]),s._v(" "),t("h3",{attrs:{id:"_5-1-slave端"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-1-slave端"}},[s._v("#")]),s._v(" 5.1 slave端")]),s._v(" "),t("blockquote",[t("p",[s._v("在windows上的settings.py文件的最后增加如下一行")])]),s._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("REDIS_HOST = 'localhost' #master IP\n\nREDIS_PORT = 6379\n\n")])])]),t("p",[s._v("配置好了远程的redis地址后启动两个爬虫（启动爬虫没有顺序限制）")]),s._v(" "),t("h3",{attrs:{id:"_6-给爬虫增加配置信息"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_6-给爬虫增加配置信息"}},[s._v("#")]),s._v(" 6 给爬虫增加配置信息")]),s._v(" "),t("div",{staticClass:"language-python extra-class"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[s._v("DUPEFILTER_CLASS "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"scrapy_redis.dupefilter.RFPDupeFilter"')]),s._v("\nSCHEDULER "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v('"scrapy_redis.scheduler.Scheduler"')]),s._v("\nSCHEDULER_PERSIST "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 是否允许暂停")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v('#SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderPriorityQueue"')]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v('#SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderQueue"')]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v('#SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderStack"')]),s._v("\n\nITEM_PIPELINES "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'example.pipelines.ExamplePipeline'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("300")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'scrapy_redis.pipelines.RedisPipeline'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("400")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n")])])]),t("h2",{attrs:{id:"_7-运行程序"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_7-运行程序"}},[s._v("#")]),s._v(" 7 运行程序")]),s._v(" "),t("h3",{attrs:{id:"_7-1-运行slave"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_7-1-运行slave"}},[s._v("#")]),s._v(" 7.1 运行slave")]),s._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("scrapy runspider 文件名.py\n")])])]),t("p",[s._v("开起没有先后顺序")]),s._v(" "),t("h3",{attrs:{id:"_7-2-运行master"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_7-2-运行master"}},[s._v("#")]),s._v(" 7.2 运行master")]),s._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("lpush (redis_key)  url #括号不用写\n")])])]),t("p",[t("strong",[s._v("说明")])]),s._v(" "),t("ul",[t("li",[s._v("这个命令是在redis-cli中运行")]),s._v(" "),t("li",[s._v("redis_key 是 spider.py文件中的redis_key的值")]),s._v(" "),t("li",[s._v("url 开始爬取地址，不加双引号")])]),s._v(" "),t("h2",{attrs:{id:"_8-数据导入到mongodb中"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_8-数据导入到mongodb中"}},[s._v("#")]),s._v(" 8 数据导入到mongodb中")]),s._v(" "),t("p",[s._v("等到爬虫结束后,如果要把数据存储到mongodb中，就应该修改master端process_items.py文件，如下")]),s._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("import redis\n\nimport pymongo\n\ndef main():\n\n    r = redis.Redis(host='192.168.31.245',port=6379,db=0)\n\n    client = pymongo.MongoClient(host='localhost', port=27017)\n\n    db = client.dmoz\n\n    sheet = db.sheet\n\n    while True:\n\n\n        source, data = r.blpop([\"dmoz:items\"])\n\n        item = json.loads(data)\n\n        sheet.insert(item)\n\nif __name__ == '__main__':\n\n    main()\n")])])]),t("h2",{attrs:{id:"_9-数据导入到mysql中"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_9-数据导入到mysql中"}},[s._v("#")]),s._v(" 9 数据导入到MySQL中")]),s._v(" "),t("p",[s._v("等到爬虫结束后,如果要把数据存储到mongodb中，就应该修改master端process_items.py文件，如下")]),s._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v('import redis\nimport pymysql\nimport json\ndef process_item():\n    r_client = redis.Redis(host="127.0.0.1",port=6379,db =0)\n    m_client = pymysql.connect(host="127.0.0.1",port=3306,user="root",passowrd="123456",db="lianjia")\n    source,data =r_client.blpop("lianjia:item")\n    item = json.loads(data)\n\n    cursor = m_client.cursor()\n    values = []\n    cursor.execute(sql,values)\n')])])])])}),[],!1,null,null,null);t.default=r.exports}}]);